{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Necessary Libraries\n",
    "import sagemaker\n",
    "import boto3 # Allow users to use AWS services like S3 and EC2\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri # Allow users to use SageMaker built-in algorithms\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.session import s3_input, Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-east-1\n"
     ]
    }
   ],
   "source": [
    "# Chekc the region of the Notebook instance\n",
    "my_region = boto3.session.Session().region_name\n",
    "print(my_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 bucket created successfully\n"
     ]
    }
   ],
   "source": [
    "# Create the S3 bucket in the same region\n",
    "bucket_name = 'bankapplication20201228' \n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "try:\n",
    "    if my_region == 'us-east-1':\n",
    "        s3.create_bucket(Bucket=bucket_name)\n",
    "    print('S3 bucket created successfully')\n",
    "except Exception as e:\n",
    "    print('S3 bucket creation error: ', e)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://bankapplication20201228/xgboost-as-a-built-in-algo/output\n"
     ]
    }
   ],
   "source": [
    "# Set an output path where the trained model will be saved\n",
    "prefix = 'xgboost-as-a-built-in-algo'\n",
    "output_path = 's3://{}/{}/output'.format(bucket_name, prefix)\n",
    "print(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: download bank_clean.csv\n"
     ]
    }
   ],
   "source": [
    "# Download The Dataset and Store it in S3\n",
    "import pandas as pd\n",
    "import urllib # URL handling module in Python\n",
    "try:\n",
    "    urllib.request.urlretrieve(\"https://d1.awsstatic.com/tmt/build-train-deploy-machine-learning-model-sagemaker/bank_clean.27f01fbbdf43271788427f3682996ae29ceca05d.csv\",\n",
    "                              \"bank_clean.csv\") # Download the dataset in the Notebook Instance\n",
    "    print('Success: download bank_clean.csv') \n",
    "except Exception as e:\n",
    "    print('Data download error: ', e)\n",
    "\n",
    "# Read the csv file and store it as a pandas data frame\n",
    "model_data = pd.read_csv('./bank_clean.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 41188 entries, 0 to 41187\n",
      "Data columns (total 61 columns):\n",
      " #   Column                         Non-Null Count  Dtype\n",
      "---  ------                         --------------  -----\n",
      " 0   age                            41188 non-null  int64\n",
      " 1   campaign                       41188 non-null  int64\n",
      " 2   pdays                          41188 non-null  int64\n",
      " 3   previous                       41188 non-null  int64\n",
      " 4   no_previous_contact            41188 non-null  int64\n",
      " 5   not_working                    41188 non-null  int64\n",
      " 6   job_admin.                     41188 non-null  int64\n",
      " 7   job_blue-collar                41188 non-null  int64\n",
      " 8   job_entrepreneur               41188 non-null  int64\n",
      " 9   job_housemaid                  41188 non-null  int64\n",
      " 10  job_management                 41188 non-null  int64\n",
      " 11  job_retired                    41188 non-null  int64\n",
      " 12  job_self-employed              41188 non-null  int64\n",
      " 13  job_services                   41188 non-null  int64\n",
      " 14  job_student                    41188 non-null  int64\n",
      " 15  job_technician                 41188 non-null  int64\n",
      " 16  job_unemployed                 41188 non-null  int64\n",
      " 17  job_unknown                    41188 non-null  int64\n",
      " 18  marital_divorced               41188 non-null  int64\n",
      " 19  marital_married                41188 non-null  int64\n",
      " 20  marital_single                 41188 non-null  int64\n",
      " 21  marital_unknown                41188 non-null  int64\n",
      " 22  education_basic.4y             41188 non-null  int64\n",
      " 23  education_basic.6y             41188 non-null  int64\n",
      " 24  education_basic.9y             41188 non-null  int64\n",
      " 25  education_high.school          41188 non-null  int64\n",
      " 26  education_illiterate           41188 non-null  int64\n",
      " 27  education_professional.course  41188 non-null  int64\n",
      " 28  education_university.degree    41188 non-null  int64\n",
      " 29  education_unknown              41188 non-null  int64\n",
      " 30  default_no                     41188 non-null  int64\n",
      " 31  default_unknown                41188 non-null  int64\n",
      " 32  default_yes                    41188 non-null  int64\n",
      " 33  housing_no                     41188 non-null  int64\n",
      " 34  housing_unknown                41188 non-null  int64\n",
      " 35  housing_yes                    41188 non-null  int64\n",
      " 36  loan_no                        41188 non-null  int64\n",
      " 37  loan_unknown                   41188 non-null  int64\n",
      " 38  loan_yes                       41188 non-null  int64\n",
      " 39  contact_cellular               41188 non-null  int64\n",
      " 40  contact_telephone              41188 non-null  int64\n",
      " 41  month_apr                      41188 non-null  int64\n",
      " 42  month_aug                      41188 non-null  int64\n",
      " 43  month_dec                      41188 non-null  int64\n",
      " 44  month_jul                      41188 non-null  int64\n",
      " 45  month_jun                      41188 non-null  int64\n",
      " 46  month_mar                      41188 non-null  int64\n",
      " 47  month_may                      41188 non-null  int64\n",
      " 48  month_nov                      41188 non-null  int64\n",
      " 49  month_oct                      41188 non-null  int64\n",
      " 50  month_sep                      41188 non-null  int64\n",
      " 51  day_of_week_fri                41188 non-null  int64\n",
      " 52  day_of_week_mon                41188 non-null  int64\n",
      " 53  day_of_week_thu                41188 non-null  int64\n",
      " 54  day_of_week_tue                41188 non-null  int64\n",
      " 55  day_of_week_wed                41188 non-null  int64\n",
      " 56  poutcome_failure               41188 non-null  int64\n",
      " 57  poutcome_nonexistent           41188 non-null  int64\n",
      " 58  poutcome_success               41188 non-null  int64\n",
      " 59  y_no                           41188 non-null  int64\n",
      " 60  y_yes                          41188 non-null  int64\n",
      "dtypes: int64(61)\n",
      "memory usage: 19.5 MB\n"
     ]
    }
   ],
   "source": [
    "# Check the basic information about the columns in the dataset\n",
    "model_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28831, 61) (12357, 61)\n"
     ]
    }
   ],
   "source": [
    "# Train/Test Split\n",
    "import numpy as np\n",
    "train_data, test_data = np.split(model_data.sample(frac=1, random_state = 999), [int(0.7*len(model_data))])\n",
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save traininng dataset into S3 bucket\n",
    "import os\n",
    "# Notice the single bracket here of selecting the 'y_yes' column, which produces a series in Pandas\n",
    "pd.concat([train_data['y_yes'], train_data.drop(['y_no', 'y_yes'], axis=1)], \n",
    "          axis=1).to_csv('train.csv', index=False, header=False)\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save testing dataset into S3 bucket\n",
    "pd.concat([test_data['y_yes'], test_data.drop(['y_no', 'y_yes'], axis=1)], \n",
    "          axis=1).to_csv('test.csv', index=False, header=False)\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'test/test.csv')).upload_file('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sagemaker.inputs.TrainingInput'>\n",
      "<class 'sagemaker.inputs.TrainingInput'>\n"
     ]
    }
   ],
   "source": [
    "# Read the training dataset and test dataset from S3 bucket\n",
    "s3_input_train = sagemaker.TrainingInput(s3_data='s3://{}/{}/train'.format(bucket_name, prefix), content_type='csv')\n",
    "s3_input_test = sagemaker.TrainingInput(s3_data ='s3://{}/{}/test'.format(bucket_name,prefix),content_type='csv')\n",
    "print(type(s3_input_train))\n",
    "print(type(s3_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Search for the XGBoost Training Images from the Elastic Container Repository\n",
    "# Use :1 version to ensure the stable version\n",
    "container = retrieve(framework = 'xgboost',region = my_region, version = '1.0-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize hyperparameters\n",
    "hyperparameters = {\n",
    "    \"max_depth\":\"5\",\n",
    "    \"eta\":\"0.2\",\n",
    "    \"gamma\":\"4\",\n",
    "    \"min_child_weight\":\"6\",\n",
    "    \"subsample\":\"0.7\",\n",
    "    \"objective\":\"binary:logistic\",\n",
    "    \"num_round\":50\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a SageMaker estimator that calls the xgboost-container\n",
    "estimator = sagemaker.estimator.Estimator(image_uri=container, \n",
    "                                          hyperparameters=hyperparameters,\n",
    "                                          role=sagemaker.get_execution_role(),\n",
    "                                          instance_count=1, \n",
    "                                          instance_type='ml.m5.2xlarge', \n",
    "                                          volume_size=5, # 5 GB \n",
    "                                          output_path=output_path,\n",
    "                                          use_spot_instances=True,\n",
    "                                          max_run=300,\n",
    "                                          max_wait=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-30 06:54:31 Starting - Starting the training job...\n",
      "2020-12-30 06:54:54 Starting - Launching requested ML instancesProfilerReport-1609311270: InProgress\n",
      "......\n",
      "2020-12-30 06:56:01 Starting - Preparing the instances for training.........\n",
      "2020-12-30 06:57:16 Downloading - Downloading input data\n",
      "2020-12-30 06:57:16 Training - Downloading the training image..\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[06:57:38] 28831x59 matrix with 1701029 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[06:57:38] 12357x59 matrix with 729063 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 28831 rows\u001b[0m\n",
      "\u001b[34mINFO:root:Validation matrix has 12357 rows\u001b[0m\n",
      "\u001b[34m[06:57:38] WARNING: /workspace/src/learner.cc:328: \u001b[0m\n",
      "\u001b[34mParameters: { num_round } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.10111#011validation-error:0.10237\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.10055#011validation-error:0.10294\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.10010#011validation-error:0.10229\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.10055#011validation-error:0.10189\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.10034#011validation-error:0.10237\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.10017#011validation-error:0.10278\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.10007#011validation-error:0.10269\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.10014#011validation-error:0.10237\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.09979#011validation-error:0.10253\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.09975#011validation-error:0.10269\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.09958#011validation-error:0.10245\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.09961#011validation-error:0.10229\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.09955#011validation-error:0.10269\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.09955#011validation-error:0.10294\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.09972#011validation-error:0.10286\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.09941#011validation-error:0.10310\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.09944#011validation-error:0.10310\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.09934#011validation-error:0.10286\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.09913#011validation-error:0.10278\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.09913#011validation-error:0.10229\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.09903#011validation-error:0.10237\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.09885#011validation-error:0.10253\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.09875#011validation-error:0.10229\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.09896#011validation-error:0.10221\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.09875#011validation-error:0.10189\u001b[0m\n",
      "\u001b[34m[25]#011train-error:0.09864#011validation-error:0.10221\u001b[0m\n",
      "\u001b[34m[26]#011train-error:0.09875#011validation-error:0.10213\u001b[0m\n",
      "\u001b[34m[27]#011train-error:0.09857#011validation-error:0.10197\u001b[0m\n",
      "\u001b[34m[28]#011train-error:0.09840#011validation-error:0.10197\u001b[0m\n",
      "\u001b[34m[29]#011train-error:0.09830#011validation-error:0.10197\u001b[0m\n",
      "\u001b[34m[30]#011train-error:0.09799#011validation-error:0.10229\u001b[0m\n",
      "\u001b[34m[31]#011train-error:0.09795#011validation-error:0.10253\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.09795#011validation-error:0.10237\u001b[0m\n",
      "\u001b[34m[33]#011train-error:0.09792#011validation-error:0.10229\u001b[0m\n",
      "\u001b[34m[34]#011train-error:0.09799#011validation-error:0.10253\u001b[0m\n",
      "\u001b[34m[35]#011train-error:0.09805#011validation-error:0.10245\u001b[0m\n",
      "\u001b[34m[36]#011train-error:0.09812#011validation-error:0.10237\u001b[0m\n",
      "\u001b[34m[37]#011train-error:0.09816#011validation-error:0.10229\u001b[0m\n",
      "\u001b[34m[38]#011train-error:0.09795#011validation-error:0.10237\u001b[0m\n",
      "\u001b[34m[39]#011train-error:0.09795#011validation-error:0.10237\u001b[0m\n",
      "\u001b[34m[40]#011train-error:0.09764#011validation-error:0.10221\u001b[0m\n",
      "\u001b[34m[41]#011train-error:0.09743#011validation-error:0.10253\u001b[0m\n",
      "\u001b[34m[42]#011train-error:0.09767#011validation-error:0.10261\u001b[0m\n",
      "\u001b[34m[43]#011train-error:0.09774#011validation-error:0.10286\u001b[0m\n",
      "\u001b[34m[44]#011train-error:0.09771#011validation-error:0.10302\u001b[0m\n",
      "\u001b[34m[45]#011train-error:0.09753#011validation-error:0.10294\u001b[0m\n",
      "\u001b[34m[46]#011train-error:0.09739#011validation-error:0.10294\u001b[0m\n",
      "\u001b[34m[47]#011train-error:0.09726#011validation-error:0.10286\u001b[0m\n",
      "\u001b[34m[48]#011train-error:0.09743#011validation-error:0.10278\u001b[0m\n",
      "\u001b[34m[49]#011train-error:0.09733#011validation-error:0.10294\u001b[0m\n",
      "\n",
      "2020-12-30 06:58:01 Uploading - Uploading generated training model\n",
      "2020-12-30 06:58:01 Completed - Training job completed\n",
      "Training seconds: 59\n",
      "Billable seconds: 22\n",
      "Managed Spot Training savings: 62.7%\n"
     ]
    }
   ],
   "source": [
    "# Start the model training\n",
    "estimator.fit({'train': s3_input_train, 'validation': s3_input_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------!"
     ]
    }
   ],
   "source": [
    "# Deploy the model using SageMaker Hosting Service\n",
    "xgb_predictor = estimator.deploy(initial_instance_count=1,instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sagemaker.predictor.Predictor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(xgb_predictor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12357,)\n"
     ]
    }
   ],
   "source": [
    "# Prediction of the test data using the deployed model\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "test_data_array = test_data.drop(['y_no', 'y_yes'], axis=1).values #load the data into an array\n",
    "xgb_predictor.serializer = CSVSerializer() # set the serializer type\n",
    "predictions = xgb_predictor.predict(test_data_array).decode('utf-8') # predict!\n",
    "predictions_array = np.fromstring(predictions[1:], sep=',') # and turn the prediction into an array\n",
    "print(predictions_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09626938, 0.0393134 , 0.10125767, ..., 0.05990799, 0.0801229 ,\n",
       "       0.31911257])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Classification Rate: 89.7%\n",
      "\n",
      "Predicted      No Purchase    Purchase\n",
      "Observed\n",
      "No Purchase    91% (10795)    37% (172)\n",
      "Purchase        9% (1100)     63% (290) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate the confusion matrix for the test dataset result\n",
    "cm = pd.crosstab(index=test_data['y_yes'], columns=np.round(predictions_array), rownames=['Observed'], colnames=['Predicted'])\n",
    "tn = cm.iloc[0,0]; fn = cm.iloc[1,0]; tp = cm.iloc[1,1]; fp = cm.iloc[0,1]; p = (tp+tn)/(tp+tn+fp+fn)*100\n",
    "print(\"\\n{0:<20}{1:<4.1f}%\\n\".format(\"Overall Classification Rate: \", p))\n",
    "print(\"{0:<15}{1:<15}{2:>8}\".format(\"Predicted\", \"No Purchase\", \"Purchase\"))\n",
    "print(\"Observed\")\n",
    "print(\"{0:<15}{1:<2.0f}% ({2:<}){3:>6.0f}% ({4:<})\".format(\"No Purchase\", tn/(tn+fn)*100,tn, fp/(tp+fp)*100, fp))\n",
    "print(\"{0:<16}{1:<1.0f}% ({2:<}){3:>7.0f}% ({4:<}) \\n\".format(\"Purchase\", fn/(tn+fn)*100,fn, tp/(tp+fp)*100, tp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the endpoints and clear the objects in the S3 bucket\n",
    "sagemaker.Session().delete_endpoint(xgb_predictor.endpoint_name)\n",
    "bucket_to_delete = boto3.resource('s3').Bucket(bucket_name)\n",
    "bucket_to_delete.objects.all().delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
